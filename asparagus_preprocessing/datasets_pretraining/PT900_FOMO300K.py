import os
from asparagus_preprocessing.configs.preprocessing_presets import (
    get_noresampling_preprocessing_config,
    get_FOMO300K_saving_config,
)
from asparagus_preprocessing.paths import get_data_path, get_source_path
from asparagus_preprocessing.utils.metadata_generation import generate_dataset_json, combine_datasets_without_splits
from asparagus_preprocessing.utils.saving import enhanced_save_json
from asparagus_preprocessing.utils.dataclasses import DatasetConfig
from asparagus_preprocessing.utils.loading import load_json
from asparagus_preprocessing.utils.detect import find_processed_dataset


def main(
    path: str = "",
    subdir: str = "",
    processes=12,
    bidsify=False,
    save_dset_metadata=False,
    save_as_tensor=False,
):
    dataset_collection = [
        "PT001",
        "PT002",
        "PT003",
        "PT004",
        "PT005",
        "PT006",
        "PT007",
        "PT008",
        "PT009",
        "PT010",
        "PT011",
        "PT012",
        "PT013",
        "PT014",
        "PT015",
        "PT016",
        "PT017",
        "PT018",
        "PT019",
        "PT020",
        "PT021",
        "PT022",
        "PT023",
        "PT024",
        "PT025",
        "PT026",
        "PT027",
        "PT028",
        "PT029",
        "PT030",
        "PT031",
        "PT032",
        "PT033",
        "PT034",
        "PT035",
        "PT036",
        "PT037",
    ]
    dataset_config = DatasetConfig(
        task_name="PT900_FOMO300K",
        n_classes=1,
        n_modalities=1,
        in_extensions=[],
        split=None,
        patterns_exclusion=[],
        patterns_DWI=[],
        patterns_PET=[],
        patterns_perfusion=[],
        patterns_m0=[],
        patterns_bidsify=[],
        df_columns=[],
    )
    preprocessing_config = get_noresampling_preprocessing_config()
    saving_config = get_FOMO300K_saving_config(
        save_as_tensor=save_as_tensor, save_dset_metadata=save_dset_metadata, bidsify=bidsify
    )

    target_dir = os.path.join(get_data_path(), dataset_config.task_name)
    os.makedirs(target_dir, exist_ok=True)

    all_dataset_json, all_files = combine_datasets_without_splits(dataset_collection)

    generate_dataset_json(
        os.path.join(target_dir, "dataset.json"),
        dataset_name=dataset_config.task_name,
        preprocessing_config=preprocessing_config,
        saving_config=saving_config,
        dataset_config=dataset_config,
        metadata={
            "dataset_collection_jsons": all_dataset_json,
            "dataset_collection": dataset_collection,
            "final_files": len(all_files),
        },
    )
    enhanced_save_json(all_files, os.path.join(target_dir, "paths.json"))


if __name__ == "__main__":
    args = asparagus_parser.parse_args()
    main(
        processes=args.num_workers,
        bidsify=args.bidsify,
        save_dset_metadata=args.save_dset_metadata,
        save_as_tensor=args.save_as_tensor,
    )
