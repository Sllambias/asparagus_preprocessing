import os
from asparagus_preprocessing.utils.detect import recursive_find_and_group_files, get_bvals_and_bvecs_v1, recursive_find_files
from asparagus_preprocessing.configs.preprocessing_presets import (
    get_noresampling_preprocessing_config,
    get_FOMO300K_saving_config,
)
from asparagus_preprocessing.utils.metadata_generation import postprocess_standard_dataset
from asparagus_preprocessing.paths import get_data_path, get_source_path
from asparagus_preprocessing.utils.path import get_image_output_paths
from asparagus_preprocessing.utils.mp import multiprocess_mri_dwi_pet_perf_cases
from asparagus_preprocessing.utils.parser import asparagus_parser
from asparagus_preprocessing.utils.dataclasses import DatasetConfig
from asparagus_preprocessing.utils.bids import extract_demographics, rename_files_with_mapping


def concat_demographic_files(source_dir, out_path, column_mapping=None):
    import pandas as pd

    all_dfs = []
    for root, dirs, files in os.walk(source_dir):
        for f in files:
            if f.endswith(".tsv"):
                df = pd.read_csv(os.path.join(root, f), sep="\t")
            
                # Handle age calculation if both testageyears and testagemonth exist
                if "testageyears" in df.columns and "testagemonth" in df.columns:
                    # Convert months (0-11) to decimal fraction of year
                    df["age_at_scan_1"] = df["testageyears"] + (df["testagemonth"] / 12.0)
                    # Remove the original columns after combining
                    df = df.drop(columns=["testageyears", "testagemonth"])
                elif "testageyears" in df.columns:
                    # If only years exist, just rename
                    df = df.rename(columns={"testageyears": "age_at_scan_1"})

                # Apply other column mapping if provided
                if column_mapping:
                    df = df.rename(columns=column_mapping)

                all_dfs.append(df)

    # Concatenate all dataframes
    combined_df = pd.concat(all_dfs, ignore_index=True)

    # Drop duplicates - adjust columns based on what's actually available
    duplicate_cols = ["participant_id"]
    for col in ["age_at_scan_1", "sex", "handedness", "group"]:
        if col in combined_df.columns:
            duplicate_cols.append(col)

    combined_df.drop_duplicates(subset=duplicate_cols).to_csv(out_path, index=False)


def main(
    path: str = get_source_path(),
    subdir: str = "ACPI",
    processes=12,
    bidsify=False,
    save_dset_metadata=False,
    save_as_tensor=False,
):
    saving_config = get_FOMO300K_saving_config(
        save_as_tensor=save_as_tensor, save_dset_metadata=save_dset_metadata, bidsify=bidsify
    )
    preprocessing_config = get_noresampling_preprocessing_config()

    process(
        saving_config=saving_config,
        preprocessing_config=preprocessing_config,
        path=path,
        subdir=subdir,
        processes=processes,
    )


def process(
    saving_config,
    preprocessing_config,
    path: str = get_source_path(),
    subdir: str = "ACPI",
    processes=12,
    task_name: str = "PT004_ACPI",
):
    dataset_config = DatasetConfig(
        task_name=task_name,
        n_classes=1,
        n_modalities=1,
        in_extensions=[".nii.gz"],
        split=None,
        patterns_exclusion=[],
        patterns_DWI=[],
        patterns_PET=[],
        patterns_perfusion=[],
        patterns_m0=[],
        patterns_bidsify=[r"sub-([0-9]{7})"],
        df_columns=["participant_id", "session_id", "age", "sex", "handedness", "group"],
    )

    source_dir = os.path.join(path, subdir)
    target_dir = os.path.join(get_data_path(), dataset_config.task_name)
    os.makedirs(target_dir, exist_ok=True)

    files_standard, files_DWI, files_PET, files_Perf, files_excluded = recursive_find_and_group_files(
        source_dir,
        extensions=dataset_config.in_extensions,
        patterns_dwi=dataset_config.patterns_DWI,
        patterns_pet=dataset_config.patterns_PET,
        patterns_perfusion=dataset_config.patterns_perfusion,
        patterns_exclusion=dataset_config.patterns_exclusion,
        processes=processes,
    )

    files_standard_out = get_image_output_paths(files_standard, source_dir, target_dir, dataset_config.in_extensions)
    files_DWI_out = get_image_output_paths(files_DWI, source_dir, target_dir, dataset_config.in_extensions)
    files_PET_out = get_image_output_paths(files_PET, source_dir, target_dir, dataset_config.in_extensions)
    files_Perf_out = get_image_output_paths(files_Perf, source_dir, target_dir, dataset_config.in_extensions)
    bvals_DWI, bvecs_DWI = get_bvals_and_bvecs_v1(files_DWI, dataset_config.in_extensions)

    multiprocess_mri_dwi_pet_perf_cases(
        files_standard=files_standard,
        files_standard_out=files_standard_out,
        files_DWI=files_DWI,
        bvals_DWI=bvals_DWI,
        bvecs_DWI=bvecs_DWI,
        files_DWI_out=files_DWI_out,
        files_PET=files_PET,
        files_PET_out=files_PET_out,
        files_Perf=files_Perf,
        files_Perf_out=files_Perf_out,
        patterns_m0=dataset_config.patterns_m0,
        preprocessing_config=preprocessing_config,
        saving_config=saving_config,
        processes=processes,
        strict=False,
    )
    if saving_config.bidsify or saving_config.save_dset_metadata:
        # Concatenate all the csv files in between sessions
        demographics_csv_path = os.path.join(target_dir, "tmp.csv")
        column_mapping = {
            "sexmf": "sex",
        }
        custom_json_mapping = {
            "du_1/": os.path.join(source_dir, "ACPI_raw", "du_1", "T1w.json"),
            "mta_1/": os.path.join(source_dir, "ACPI_raw", "mta_1", "T1w.json"),
        }
        concat_demographic_files(source_dir, demographics_csv_path, column_mapping)
        processed_files = recursive_find_files(target_dir, extensions=dataset_config.in_extensions + [".pt"])
        expanded_df, subjects_df, mri_info_df = extract_demographics(
            processed_files=processed_files,
            demographics_csv_path=demographics_csv_path,
            columns_to_keep=dataset_config.df_columns,
            custom_patterns=dataset_config.patterns_bidsify,
            handedness_mapping={1: "R", 2: "L", 3: "A"},
            custom_json_mapping=custom_json_mapping,
        )
        os.remove(demographics_csv_path)

    if saving_config.bidsify:
        subjects_df.to_csv(os.path.join(target_dir, "participants.tsv"), sep='\t', index=False)

        mapping_df = rename_files_with_mapping(
            target_dir=target_dir,
            expanded_df=expanded_df,
        )

    if saving_config.bidsify or saving_config.save_dset_metadata:
        mri_info_df.to_csv(os.path.join(target_dir, "mri_info.tsv"), sep='\t', index=False)

    postprocess_standard_dataset(
        dataset_config=dataset_config,
        preprocessing_config=preprocessing_config,
        saving_config=saving_config,
        target_dir=target_dir,
        source_files_standard=files_standard,
        source_files_DWI=files_DWI,
        source_files_PET=files_PET,
        source_files_Perf=files_Perf,
        source_files_excluded=files_excluded,
        processes=processes,
    )


if __name__ == "__main__":
    args = asparagus_parser.parse_args()
    main(
        processes=args.num_workers,
        bidsify=args.bidsify,
        save_dset_metadata=args.save_dset_metadata,
        save_as_tensor=args.save_as_tensor,
    )
